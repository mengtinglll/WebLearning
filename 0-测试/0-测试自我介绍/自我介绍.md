- [自我介绍](#自我介绍)
  - [实习经历 python工程师](#实习经历-python工程师)
    - [年报（新浪财经）](#年报新浪财经)
    - [论文（知网）](#论文知网)
    - [文本摘要](#文本摘要)
  - [项目经历 前端工程师](#项目经历-前端工程师)
    - [缓存](#缓存)
    - [技术选型](#技术选型)
  - [遇到的困难及解决方案](#遇到的困难及解决方案)
    - [爬虫](#爬虫)
      - [封ip](#封ip)
      - [断点续传](#断点续传)
      - [断连](#断连)
      - [重复下载](#重复下载)
      - [改进点](#改进点)
    - [前端](#前端)
  - [项目中的模块化、组件化开发思想（场景，原因，结果）](#项目中的模块化组件化开发思想场景原因结果)
  - [项目中涉及的测试](#项目中涉及的测试)
    - [selenium](#selenium)
    - [postman](#postman)
## 自我介绍
- 你好，我叫李梦亭，现在是厦门大学信息学院计算机技术专业研二的学生。
- 研一上学期主要就是认真学习专业课，同担任班级团支书，有意识地去提高自己的沟通和解决的能力
- 研一下学期进入爱德力智能科技有限公司兼职实习，当时的岗位是数据工程师，涉及到的项目是的智能财务问答系统，项目的目标是通过程序自动分析财务报表，得到公司现状分析，并结合论文库构建的知识图谱，为企业提供解决方案。

- 我在其中负责数据采集和预处理工作。
- 在数据挖掘获取数据的工作中，我使用python语言，通过urllib、selenium等技术爬取了中国所有上市公司近5年的财报、知网中财务管理相关的论文100+G。
- 在数据预处理的工作中，我设计并实现了文本摘要算法对论文内容提炼，为后续构建知识图谱提供数据基础。

- 到了研二开始我加入到实验室的项目中，我们实验室的项目主要是面向web开发。
- 首先接触的是一个公益教育平台的项目中，学生可以在平台上发布问题，教师则在平台上回答问题或进行课程直播。我参与了项目需求分析，并负责了部分前端开发任务，设计实现首页、注册页及个人页面。
- 后来加入一个新的项目，叫做好农圈，好农圈是一个类似于小红书的果蔬产品社交分享平台。
- 而好农圈后台管理系统就是用来管理好农圈用户、圈子以及帖子的，进行审核以及权限修改的操作，也便于管理员查看系统运行情况。
- 我在其中担任好农圈后台管理系统的前端负责人。负责包括技术选型、框架构建、公共组件库开发等总体设计任务，同时在设计中关注项目性能，通过缓存技术减少请求，提升用户体验的同时减轻了服务器的压力。
  
- 总的来说， 我工作细致踏实，有良好的编程习惯，较强的编程能力，以及一定的项目经验，希望求得测试开发工程师的岗位
### 实习经历 python工程师
#### 年报（新浪财经）
  1. urllib（请求） + re（匹配）+ 代理ip池、请求头池
  2. 【流程】
      - 首先我手上是有所有股票代码的一个列表，目标是去爬取所有上市公司的企业年报。
      - 经过对比几个财经网站，我选定新浪财经作为目标网站，因为它的规律比较明显的，所有上市公司的年报列表页面的url是相似的，仅仅是股票代码这一个参数的不同。由此，我就获得了新浪财经上所有上市公司的年报列表页面的url，这是爬虫的第一层
      - 接下来就是单个公司的年报列表页面的爬取，通过分析页面结构，发现在id=datelist的节点下，所有a标签就是具体的近几年的年报的详情页链接。至此是爬虫的第二层。
      - 进入到xx年年报的详情页后，通过正则匹配找到（符合一定规律）且以.pdf结尾的url（xxxx.*?.pdf）,就可以下载该年份的年报了，这是爬虫的第三层，也是最后一层。
      - 根据这个逻辑，爬虫的代码很容易就可以写出来，但是会遇到一些反爬虫的限制，比如被封ip的问题
  3. 【代理ip】
  4. 【log日志】
      - 对python的logging库进行了简单的二次封装
      - init函数，对log实例进行初始化处理，主要涉及输出log等级=debug、地址、文件名
      - 分为logger.info和logger.error两种，在多处埋点帮助实现【断点续传】
#### 论文（知网）
  1. selenium（模拟） + threading（多线程）
  2. 【流程】
      - 首先我手上是有一个财务管理领域的关键词列表（平衡记分卡、差异分析等），目标是根据这些关键词去爬取知网上的相关论文，并设置一些筛选条件（必须为核心期刊以上）
      - 接下去具体爬虫的过程就和年报的爬取相差不大了（搜索筛选；第一层：页码；第二层：列表；第三层：详情下载）
      - 不同的是知网是通过校园vpn实现登录的，不能通过代理ip的方式来避免被封，所以只能降低爬虫的速度。但这又会导致效率很低，所以我是使用了多线程的方式来解决这个问题。
  3. 【selenium】
      - 一开始都是通过find_element_by_xpath来定位的
      - 但是跑了一段时间以后发现，有报错找不到该xpath
      - 【定位】发现是比较老的论文下载页面也会有些许不同，xpath也不同，所以就用了try...except...来解决
      - 【解决】过了一段时间又发现更老的页面的下载按钮又是不同的xpath。我对比发现他们的id都是‘pdfdown’，所以最后改成了find_element_by_id的方式，这个问题就没再出现了。
  4. 【threading】
      - 【场景】由于要避免被知网封（输验证码），除了更换请求头外，还需要通过sleep减慢操作速度，这就导致了效率低下
      - 【解决】所以我采样多线程的方式去解决它。
      - 但是其中还是有两个小问题，一个是知网的登录容易断，减慢爬虫的速度相当于拉长战线，提高了断连出现的概率。另一个是每次断连后重新登录、搜索返回的列表顺序是不同的，如果直接从上次下载失败的第几页第几条开始继续爬虫，会有重复下载和遗漏的现象。
      - 我的解决方案是一次性获取所有的下载链接，将获取链接和下载论文拆分开来，既避免了重复、遗漏下载的问题，也由于减少了selenium操作的时间，也就避免了断连出现的概率，一举两得。而且获取下载地址的步骤是不需要登录的，所以可以通过代理ip+多线程的方式，快速的获取所有下载地址。
  5. 【log】
      - 方便定位问题 
#### 文本摘要
1. 其实公司开始是想要通过知识图谱的方式去做文本摘要的，但是由于负责算法的人员进展太缓慢，所以让我用传统简单的方式去实现基础的摘要功能。
2. 文本摘要算法主要分为抽取式和生成式两种，我在项目中采用的是抽取式的方法，根据线索词、位置信息、关键词信息对句子进行评分，最后根据评分选取top20%的句子作为文本摘要的结果。
3. 首先对文章进行打标签，拆分段落，为每个句子标识【段】【句】
4. 对每句话进行处理，用结巴分词，然后去除停用词。
5. 根据pageRank算法的思想，统计高频词。
6. 线索词
7. 根据以上三个信息，平衡权重，计算句子得分
### 项目经历 前端工程师
#### 缓存
1. 【图片预加载】
   圈子列表图片的预加载放在了首页执行，也就是在加载圈子列表页面之前，就把他需要的图片进行了预加载，之后用户进入页面，就可以达到图片秒现的一个效果。但直接放入首页可能会影响首页的加载速度，所以我在这边还加了一个监听，也就是在首页加载完成后，才去执行预加载
```
    function preloader() {
    if (document.images) {
        var img1 = new Image();
        var img2 = new Image();
        var img3 = new Image();
        img1.src = "http://domain.xxx.gif";
        img2.src = "http://domain.yyy.gif";
        img3.src = "http://domain.zzz.gif";
    }
  }
  function addLoadEvent(func) {
    var oldonload = window.onload;
    if (typeof window.onload != 'function') {
        window.onload = func;
    } else {
        window.onload = function() {
            if (oldonload) {
                oldonload();
            }
            func();
        }
    }
}
addLoadEvent(preloader);
```
    - 【首次请求】创建Image对象，就相当于发送了一个`get`请求，服务器在返回这个资源的同时，在response的header加上强制缓存的Cache-Control（或expires）和协商缓存的last-modify（Etag）的header，如：`cache-control: max-age=86400`。
    - 根据服务器响应头中设置的浏览器就进行了缓存（cache/disk），把这个资源连同所有response header一起缓存下来
    - 【再次请求】浏览器再请求这个资源时，先从缓存中寻找，找到这个资源后，根据它第一次的请求时间和Cache-Control设定的有效期，计算出一个资源过期时间，再拿这个过期时间跟当前的请求时间比较，如果请求时间在过期时间之前，就能命中缓存，否则就不行；
    - 如果强制缓存没有命中，浏览器向服务器发送请求（带If-Modified-Since/If-None-Match），
    - 服务器根据If-Modified-Since判断资源是否过期，未过期返回304（不再带lastmodify，但带Etag）Cache-Control Header在重新加载的时候会被更新；
    - 


1. 【图片懒加载】
2. 【防抖】
    场景：用户注册时，需要验证用户名是否已被占用，如果采用onBlur失焦时才向后台发请求验证，会带来不好的用户体验。但如果使用onChange，则会造成频繁的请求，造成资源的浪费。
    解决：故使用防抖解决，用户每次输入后延时300毫秒，再向后台发请求。
    ```
    export const debounce = (cb, delay = 1000) => {
    let timer = null;
    return function (...args) {
        const context = this;
        if (timer) clearTimeout(timer);
        timer = setTimeout(() => {
            cb.apply(context, args);
            timer = null;
        }, delay);
    }
    }
    //  绑定请求
    this.callAjax = debounce(this.callAjax,1000);
    ```
3. 【浏览器缓存】
   将分类列表/地址列表映射等不会变化的，在多处用到的数据，存在localstorage中。
#### 技术选型
1. 【团队因素】由于实验室一直是使用react系列
2. 【】

### 遇到的困难及解决方案
#### 爬虫
遇到的困难有：
##### 封ip
1. 爬取新浪财经网站上市公司年报过程中，遇到的问题就是爬虫过程被异常终止，原因主要是IP被封。  
【解决方案】  
解决方法有两个，一个是降低爬虫速度，另一个是使用IP代码，后者明显是较优的方案。遇到ip被封的情况，直接替换一个代理ip，我通过维护一个代理IP池供爬虫过程中使用解决了该问题。  
（如果需要介绍IP代理池的思路）：从西刺网站爬取代理IP，从IP池随机获取一个IP并进行验证，验证通过则使用，验证不通过则丢弃，直到IP池资源耗尽。项目中，IP池足够大，足够使用，不需要考虑IP池耗尽之后的处理。
##### 断点续传
2. 除了解决IP被封，爬虫过程被意外中止之后，还遇到一个问题是重新开启爬虫任务时，如何从上次终止的地方开始继续爬取，相当于要实现一个"断点续传"的功能。  
【解决方案】  
需要爬取的上市公司有一个清单，相当于爬虫终止条件是爬取了该清单的所有上市公司，我的方案是记录log日志，一方面可以根据日志分析程序终止的原因，另一方便也可以记录爬虫记录，以便爬虫从上次终止的地方继续。  
##### 断连
3. 在爬取知网论文项目中，遇到的主要问题是性能问题，知网爬取使用校内VPN，无法使用代理，因此只能通过随机延时的方式避免`过度下载`而被封。但是随机延时会让这个爬虫过程变慢。  
【解决方案】  
为了减小延时时间，通过退出重新登录、关闭浏览器重新开启、重新搜索关键字等方式可以避免被封。
##### 重复下载
4. 在爬取知网论文项目中，遇到的另外一个问题就是爬取结果重复。使用退出重新登录，重新搜索来避免反爬虫时，发现前后两次检索关键字的结果可能不一致的，这就会导致有些论文重复下载，有些论文未被下载。  
【解决方案】  
分析爬虫过程，发现主要的时间都是花费在`随机延时`过程，因此转变了思路，将爬虫过程分为两个步骤：  
1、先搜索关键字，获取关键字结果的所有论文下载地址，这个过程因为不涉及论文下载，因此不需要延时，可能很快速完成。由于该操作在一次搜索完成，获取到的下载链接也是无重复，无遗漏的。
2、步骤1获取到了`无重复`结果的下载链接后，就可以使用`多线程`下载来优化问题3了。将下载链接存入队列中，每个线程各自向队列去获取下载链接，直接下载。原本的`随机延时`操作可省略，只要在爬取一定次数后，进行退出登录，退出浏览器的操作，就可以避免被封。
【面试官可能问的问题】  
爬虫过程是否考虑使用`scrapy`等爬虫框架？考虑过，但是`scrapy`等通用爬虫框架有特定的流程，流程无法灵活
##### 改进点
1. 【定时更新】 爬取年报的项目，当时项目进行的时间是3，4月份，到了5，6月份新的年报又更新了。
   - Windows有个任务计划程序（有可视界面，也有指令schtasks /create /），可以设置触发器，定时执行指定的程序。
   - linux下也有类似的，有个crontab指令可以去定时执行（定时任务的守护进程）
   - 【略】还有一些python框架可以用（celery），或者scraypy的cmdline
2. 【增量爬取】一个改进点是支持以`更新`方式爬取，将数据与数据库内数据比对，如果没有则下载新的。
3. 爬取知网论文项目，可以改进为`分布式`爬虫，在多台电脑上分别进行下载任务，进一步提高爬虫的性能。
#### 前端
- 【场景】：用户在平台的等级除了根据用户活跃度自动升级以外，还可以通过后台管理系统，由管理员给他手动升级
点击按钮后调用setState给user.level+1，如果是供应商用户，则额外再增加1级。但是我发现这个额外增加的1级不会实现。实现一个连续加两次的动画。
- 【定位】：更改成供应商额外增加0.5，发现结果只增加了0.5，所以推测注意到setState并不是同步的，对相同属性的设置只保留最后一次的设置
- 【原理】
  - 然后我就去读了react的源码，去了解setState的执行机制。
  - 为了减少重绘，react在一个渲染周期中，state的值在修改了之后并不会立即被修改，而是通过一个队列机制实现state的更新。
  - 当执行setState时，会把需要更新的state合并后放入状态队列，只有当React的合成事件执行后，将isBatchingUpdates这个flag变为false时才会执行更新，对相同属性的更新会由后者覆盖前者
- 【解决】 写在回调函数里

### 项目中的模块化、组件化开发思想（场景，原因，结果）
- 【模块化】模块化的目的在于将一个程序按照其功能拆分，分成相互独立的模块，以便于每个模块只包含与其功能相关的内容，模块之间通过接口调用。使系统更加容易维护，开发更加方便。`重点在于拆分`
- 【组件化】组件化跟模块化是很类似的，都是主要为了对一个系统做拆分，同时，组件还具有其他属性，如可替代性，通过接口访问，可重用性等`重点在于可重用性`
1. 在爬虫的项目里，我是按照模块化的思想，划分日志、配置文件、文件存储、多线程等功能模块。提高代码可读性，解耦，然后在之后的爬虫项目可以复用，第二个爬虫项目的开发时间就减少很多了
2. 在【前端模块化】方面，我目前自主设计的项目中还没有涉及模块化，但是参与维护的一个电商平台中有拆分商品浏览模块、订单模块、购物车模块、消息模块、支付模块等。解决的问题是，我们有消费者、供应商、子公司三个相联系的平台，就像搭积木一样构建就可以了，不用为每一个平台重新设计
- 【组件化】方面，前端项目都会有围护一个组件库，比如分页组件、功能按钮组件、导航组件、搜索框。因为这些组件在很多场景页面中都会用到，抽成组件可以更好的复用，减少代码的冗余，也可以在多人开发的时候保持前端样式和代码风格的统一
### 项目中涉及的测试
- 目前用过的自动化工具就是selenium和postman。
#### selenium
是我用于爬虫的工具之一，可以模拟人操作网页，自动化地操作浏览器，也相对不容易被反爬虫。我觉得selenium也可以用于后来的前端测试中（之前没有去测试的概念），模拟用户访问，检查是否全部功能正常，也可以模拟用户疯狂点击等异常操作，测试前端在商城秒杀活动等特殊场景下的性能
#### postman
 之前项目开发用postman，主要就是用于定位问题来自于前端还是后台，测试服务器是否正常工作，分析服务器返回的数据。然后，我觉得postman可以用来测试后台接口的性能，分析每一个请求消耗的时间，也可以mock数据测试后台能否承受高并发，承受的程度是多少。分析请求消耗的时间是因为要充分考虑用户体验，对于耗时长的请求，前端可以做缓存处理，减少请求次数。测试高并发的情况是因为要对此有个估计，避免服务崩掉，也可以据此做相应的服务器扩容



